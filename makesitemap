#!/usr/bin/env python3
#
# This script will automatically make a sitemap from every .html file in the
# website repository. It will travel every subdirectory (including things 
# like /static/) searching for HTML files, so you don't have to manually 
# enter them in the script.
#
# Perhaps we'll merge it with sitegen, but not right now.
#

import os
import sys
from datetime import datetime

'''
This is the final XML implementation of the sitemap we're looking to build:

<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
   <url>
      <loc>https://maluaraujoenglish.github.io/</loc>
      <lastmod>2021-01-24</lastmod>
      <changefreq>monthly</changefreq>
   </url>
</urlset>

All <url> elements will be inserted automatically.
'''

# ------------   SETTINGS ------------   #

# Debugging mode?
DEBUG = True

# Set this to your site's domain WITHOUT a trailing slash
BASE_URL = "https://www.raspberrypibrasil.com"

# How often does the site get updated?
# Available options are daily, weekly, monthly
DEFAULT_UPDATE_FREQ = "monthly"

# This is the name of your sitemap file: 
OUTFILE = "sitemap.xml"

# Add here any exceptions to crawling that you don't want indexed.
# Example: the template.html file, Google site verification HTML files
EXCEPTIONS = [
    'template.html',
    'google5d0b5ab821f0ccb4.html',
]

# ------------  /SETTINGS ------------   #

class SiteMapper:
    def __init__(self):
        self.paths = []

    def crawl(self, debug=False):
        '''
        Walk across the directory structure, adding every HTML file to
        the list of found pages (self.paths list).

        Data stored is the path and the created date.

        The os.walk() method returns tuples of (directory, [], [files]).
        '''
        for directory in os.walk('.'):
            for element in directory[2]:
                if ".html" in element:
                    # Skip the exception files:
                    if element in EXCEPTIONS:
                        if debug:
                            print("Skipping element %s found in EXCEPTIONS" %
                                element)
                        continue

                    # strip the leading '.' from the walk() method 
                    curdir = directory[0].lstrip('.')

                    if debug:
                        print("Current directory: %s" % curdir)

                    # Can't use one timestamp, due to different computers being used.
                    # use the pubdate immediately next to the index file (much better)
                    with open(directory[0] + '/pubdate', 'r') as pubd:
                        created = pubd.read()
                        if debug:
                            print("Curdir pubdate: %s" % created)

                    # if the page name is index.html, append only the
                    # directory path. Otherwise, full path.
                    if element == "index.html":
                        pagename = curdir
                    else:
                        if curdir == '/':
                            pagename = curdir + element
                        else:
                            pagename = curdir + '/' + element

                    self.paths.append((BASE_URL + pagename, created[:-1]))


        if debug:
            print("%s pages found." % len(self.paths))
            for i in self.paths:
                print(i)

    def makesitemap(self, debug=False):
        '''
        Creates the sitemap from the stored URLs in self.paths
        '''
        self.sitemap = '''<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
'''
        for row in self.paths:
            update_freq = DEFAULT_UPDATE_FREQ

            # blog and tag indices change every week (publishing):
            frag = row[0].split('/')[-1]
            if frag == 'blog' or frag == 'tags':
                update_freq = 'weekly'

            self.sitemap += """    <url>
        <loc>%s</loc>
        <lastmod>%s</lastmod>
        <changefreq>%s</changefreq>
    </url>
""" % (row[0], row[1], update_freq)

        # close <urlset> element:
        self.sitemap += "</urlset>"

        #print(self.sitemap)
        with open(OUTFILE, 'w') as sm:
            sm.write(self.sitemap)
        print("*** Sitemap published at: %s ***" % OUTFILE)


def main():
    sm = SiteMapper()
    sm.crawl(DEBUG)
    sm.makesitemap()
    return

if __name__ == "__main__":
    main()
    sys.exit(0)
